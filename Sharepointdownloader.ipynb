{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647608bb-4632-4ff3-96f7-2895434aee6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Steps before using this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b3bb7c7-4652-4355-89b4-a3f6be79d5b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1. Go to https://portal.azure.com/#view/Microsoft_AAD_RegisteredApps/ApplicationsListBlade and create a new application (Give it a name and register). You have created your Service Principal (SP).\n",
    "2. Under Manage on the left Side Bar click on API permissions and add the following APPLICATION permissions for Microsoft Graph:\n",
    "\n",
    "                        - Sites.Selected \n",
    "\n",
    "3. Once you have added the permission you will need to send Hal Sclater the following information:\n",
    "\n",
    "                    - Application Name\n",
    "                    - Site URL for the sites you want access to\n",
    "\n",
    "4. Click on Certificates & Secrets and then New Client Secret, add a description (e.g. \"Python API Access\") and select a duration that suits your scenario. Copy the generated secret and \n",
    "   save it in a safe location. You will not be able to retrieve it again.\n",
    "5. Create a secret scope either linked to a KeyVault or Databricks Secret Scope.\n",
    "6. Add the Generated Secret from step 4 to the scope.\n",
    "7. Give a Read permission to the secret scope for the Service Principle that will be running the notebook.\n",
    "8. Give the SP permissions to the directories you want to access. \n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ce85f1d3-c24b-4579-8203-3ed7c21c4530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def Downlaod_Files_From_SharePoint(\n",
    "    tenant_id: str,\n",
    "    client_id: str,\n",
    "    scope: str,\n",
    "    key: str,\n",
    "    out_dir: str,\n",
    "    site_name: str,\n",
    "    document_library: str,\n",
    "    documents = 'All', # default as all documents else give a list of documents that are requrired to be downloaded\n",
    "    version = 'False'\n",
    "    ):\n",
    "\n",
    "    import requests\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "\n",
    "    ## Get auth token\n",
    "     \n",
    "    url = f'https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token'\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded'\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': dbutils.secrets.get(scope=scope, key=key),\n",
    "        'scope': 'https://graph.microsoft.com/.default'\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, data=data)\n",
    "    access_token = response.json().get('access_token')\n",
    "\n",
    "    ## Get Site ID\n",
    "\n",
    "    site_url = f'ukpowernetworks.sharepoint.com:/sites/{site_name}'\n",
    "\n",
    "    \n",
    "    url = f'https://graph.microsoft.com/v1.0/sites/{site_url}?$select=id'\n",
    "\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Send the GET request\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    site_id = response.json()['id']\n",
    "\n",
    "    \n",
    "\n",
    "    ## Get Document Library ID\n",
    "\n",
    "    \n",
    "\n",
    "    url = f'https://graph.microsoft.com/v1.0/sites/{site_id}/drives'\n",
    "\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {access_token}',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Send the GET request\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # print(response.json()['value'][0]['name'])\n",
    "\n",
    "    for item in response.json()['value']:\n",
    "        if item['name'] == document_library:\n",
    "            library_id = item['id']\n",
    "            break\n",
    "        else:\n",
    "            print('Drive Not Found')\n",
    "\n",
    "\n",
    "    def download_document(drive_ID, item_ID, file_name, output_path):\n",
    "        download_endpoint = f'https://graph.microsoft.com/v1.0/drives/{drive_ID}/items/{item_ID}/content'\n",
    "        response = requests.get(download_endpoint, headers=headers)\n",
    "    \n",
    "        if response.status_code == 200:\n",
    "            full_output_path = os.path.join(output_path, file_name)\n",
    "            with open(full_output_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "                print(f'Successfully downloaded {file_name} to {output_path}')\n",
    "        else:\n",
    "            print(f'Error downloading {file_name}: {response.status_code} - {response.text}')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    endpoint = f'https://graph.microsoft.com/v1.0/sites/{site_id}/drives/{library_id}/root/children'\n",
    "\n",
    "\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}\n",
    "    response = requests.get(endpoint, headers=headers)\n",
    "    items_data = response.json()\n",
    "\n",
    "\n",
    "    if documents == 'All':\n",
    "        document_ids = [item['id'] for item in items_data['value'][:2]]\n",
    "        print(f'Document IDs: {document_ids}')\n",
    "        out_dir = out_dir\n",
    "\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "\n",
    "        for item in items_data['value']:\n",
    "            document_id = item['id']\n",
    "            document_name = item['name']\n",
    "            download_document(library_id, document_id, document_name, out_dir)\n",
    "    else:\n",
    "        for item in items_data['value']:\n",
    "            if item['name'] in documents:\n",
    "                document_id = item['id']\n",
    "                document_name = item['name']\n",
    "                download_document(library_id, document_id, document_name, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dbdb666f-29fc-4493-870f-725ef3c5dc49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def Downlaod_Files_From_SharePoint(\n",
    "    tenant_id: str,\n",
    "    client_id: str,\n",
    "    scope: str,\n",
    "    key: str,\n",
    "    out_dir: str,\n",
    "    site_name: str,\n",
    "    document_library: str,\n",
    "    documents='All'\n",
    "):\n",
    "    import requests\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import shutil\n",
    "    import datetime\n",
    "\n",
    "    base_api_url = 'https://graph.microsoft.com/v1.0'\n",
    "    # Auth\n",
    "    token_url = f'https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token'\n",
    "    token_data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': dbutils.secrets.get(scope=scope, key=key),\n",
    "        'scope': 'https://graph.microsoft.com/.default'\n",
    "    }\n",
    "    access_token = requests.post(token_url, data=token_data).json().get('access_token')\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "    ## Get Site ID\n",
    "    site_resp = requests.get(\n",
    "        f'{base_api_url}/sites/ukpowernetworks.sharepoint.com:/sites/{site_name}?$select=id',\n",
    "        headers=headers)\n",
    "    site_id = site_resp.json()['id']\n",
    "\n",
    "    ## Get Document Library ID\n",
    "    drives_resp = requests.get(f'{base_api_url}/sites/{site_id}/drives', headers=headers)\n",
    "    library_id = next((d['id'] for d in drives_resp.json()['value'] if d['name'] == document_library), None)\n",
    "    if not library_id:\n",
    "        print(\"Document library not found.\")\n",
    "        return\n",
    "\n",
    "    # Download function\n",
    "    def download_file(drive_id, item_id, save_path):\n",
    "        url = f'{base_api_url}/drives/{drive_id}/items/{item_id}/content'\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "                print(f\"Downloaded: {save_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {save_path} - {response.status_code}\")\n",
    "\n",
    "    ## Look through all files and add all/chosen files to a dictionary\n",
    "    file_index = {}\n",
    "\n",
    "    def recurse_items(parent_id, current_path=''):\n",
    "        endpoint = f'{base_api_url}/drives/{library_id}/items/{parent_id}/children'\n",
    "        resp = requests.get(endpoint, headers=headers)\n",
    "        if resp.status_code != 200:\n",
    "            return\n",
    "        for item in resp.json()['value']:\n",
    "            name = item['name']\n",
    "            item_id = item['id']\n",
    "            if 'folder' in item:\n",
    "                recurse_items(item_id, os.path.join(current_path, name))\n",
    "            else:\n",
    "                full_path = os.path.join(current_path, name)\n",
    "                file_index[full_path] = item_id\n",
    "\n",
    "    # Start recursion from root\n",
    "    root_id = requests.get(f'{base_api_url}/drives/{library_id}/root', headers=headers).json()['id']\n",
    "    recurse_items(root_id)\n",
    "\n",
    "    def normalize_df(df):\n",
    "        return df.sort_index(axis=1).sort_values(by=df.columns.tolist()).reset_index(drop=True)\n",
    "\n",
    "    ROOT_DIR = '/Workspace/Users/eri.krasniqi@ukpowernetworks.co.uk/excel_versioning'\n",
    "    \n",
    "\n",
    "\n",
    "    def file_comparison(file1, file2):\n",
    "        if file1.endswith('.xlsx') and file2.endswith('.xlsx'):\n",
    "            current_file = pd.read_excel(file1)\n",
    "            new_file = pd.read_excel(file2)\n",
    "            if not current_file.equals(new_file):\n",
    "                versions_dir = os.path.join(ROOT_DIR, 'versions')\n",
    "                os.makedirs(versions_dir, exist_ok=True)\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filename = os.path.basename(file1)\n",
    "\n",
    "                versioned_name = f\"{os.path.splitext(filename)[0]}_{timestamp}.xlsx\"\n",
    "                versioned_path = os.path.join(versions_dir, versioned_name)\n",
    "\n",
    "                shutil.move(file1, versioned_path)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "    ## Download Section\n",
    "    if documents == 'All':\n",
    "        for path, item_id in file_index.items():\n",
    "            download_file(library_id, item_id, os.path.join(out_dir, path))\n",
    "    else:\n",
    "        for doc in documents:\n",
    "            if doc in file_index:\n",
    "                download_file(library_id, file_index[doc], os.path.join(out_dir, doc))\n",
    "            else:\n",
    "                print(f\"File not found in SharePoint: {doc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ea8cc86-38be-4362-a837-af2111a5d3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Apply Versioning to function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a3d4940-b939-4902-9bf3-d02c59ee9f25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Versioning_enabled allows for detected changed between downloaded and stored files to be versioned into a folder ('versions').\n",
    "\n",
    "As of now the only file types which can be versioned are excel files and txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1029e7-b70f-452a-b541-293364d72eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "import openpyxl\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import datetime\n",
    "import filecmp\n",
    "\n",
    "def download_files_from_sharepoint(\n",
    "    tenant_id: str, ## tenant Id of the app\n",
    "    client_id: str, ## client Id of the app\n",
    "    scope: str,  ## scope name\n",
    "    key: str, ## key name\n",
    "    out_dir: str,  ## or os.getcwd() if you want the files to be downloaded in the current directory\n",
    "    site_name: str, ## name of SharePoint Site, spaces removed\n",
    "    document_library: str,  ## name of Document Library of interest in SharePoint site\n",
    "    documents='All',  ## default as 'all' documents, else give a list of documents that are requrired to be downloaded\n",
    "    versioning_enabled=True,  ## If true any changes in the file will be saved in a versions folder\n",
    "    structured=True, ## Keep strcuture seen in SharePoint\n",
    "    vers_dest=os.getcwd() ## Destination for the versions folder of versioning is enabled\n",
    "):\n",
    "    base_api_url = 'https://graph.microsoft.com/v1.0'\n",
    "\n",
    "    # Auth\n",
    "    token_url = f'https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token'\n",
    "    token_data = {\n",
    "        'grant_type': 'client_credentials',\n",
    "        'client_id': client_id,\n",
    "        'client_secret': dbutils.secrets.get(scope=scope, key=key),\n",
    "        'scope': 'https://graph.microsoft.com/.default'\n",
    "    }\n",
    "    access_token = requests.post(token_url, data=token_data).json().get('access_token')\n",
    "    headers = {'Authorization': f'Bearer {access_token}'}\n",
    "\n",
    "    # Get Site ID\n",
    "    site_resp = requests.get(\n",
    "        f'{base_api_url}/sites/ukpowernetworks.sharepoint.com:/sites/{site_name}?$select=id',\n",
    "        headers=headers)\n",
    "    site_id = site_resp.json()['id']\n",
    "\n",
    "    # Get Document Library ID\n",
    "    drives_resp = requests.get(f'{base_api_url}/sites/{site_id}/drives', headers=headers)\n",
    "    library_id = next((d['id'] for d in drives_resp.json()['value'] if d['name'] == document_library), None)\n",
    "    if not library_id:\n",
    "        print(\"Document library not found.\")\n",
    "        return\n",
    "\n",
    "    def download_file(drive_id, item_id):\n",
    "        url = f'{base_api_url}/drives/{drive_id}/items/{item_id}/content'\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            print(f\"Failed to download file - {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    # Look through all files and add all/chosen files to a dictionary\n",
    "    file_index = {}\n",
    "\n",
    "    def recurse_items(parent_id, current_path=''):\n",
    "        endpoint = f'{base_api_url}/drives/{library_id}/items/{parent_id}/children'\n",
    "        resp = requests.get(endpoint, headers=headers)\n",
    "        if resp.status_code != 200:\n",
    "            ## print failed to return children and what the status code is as well as explaination\n",
    "            return\n",
    "        for item in resp.json()['value']:\n",
    "            name = item['name']\n",
    "            item_id = item['id']\n",
    "            if structured == True:\n",
    "                if 'folder' in item:\n",
    "                    recurse_items(item_id, os.path.join(current_path, name))\n",
    "                else:\n",
    "                    full_path = os.path.join(current_path, name)\n",
    "                    file_index[full_path] = item_id\n",
    "            else:\n",
    "                if 'folder' in item:\n",
    "                    recurse_items(item_id,name)\n",
    "                else:\n",
    "                    full_path = name\n",
    "                    file_index[full_path] = item_id\n",
    "\n",
    "    root_id = requests.get(f'{base_api_url}/drives/{library_id}/root', headers=headers).json()['id']\n",
    "    recurse_items(root_id)\n",
    "\n",
    "\n",
    "    def version(target_file):\n",
    "        versions_dir = os.path.join(vers_dest, 'versions')\n",
    "        os.makedirs(versions_dir, exist_ok=True)\n",
    "\n",
    "        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = os.path.basename(target_file)\n",
    "\n",
    "        versioned_name = f\"{os.path.splitext(filename)[0]}_{timestamp}.xlsx\"\n",
    "\n",
    "\n",
    "        ###############################################################\n",
    "        ## cap the num of versions by date/count (add param to func) ##\n",
    "        ###############################################################\n",
    "\n",
    "\n",
    "\n",
    "        versioned_path = os.path.join(versions_dir, versioned_name)\n",
    "\n",
    "        # Move the old file to versions folder\n",
    "        shutil.move(target_file, versioned_path)\n",
    "        print(f\"Moved {target_file} to {versioned_path}\")\n",
    "\n",
    "    # File comparison function\n",
    "    # for excel file comparison they are converted to tables and then compared\n",
    "    # comparison is made prior to writing of file in order not to overwrite the file you are comparing\n",
    "    def file_comparison(downloaded_content, target_file):\n",
    "        if target_file.endswith('.xlsx'):\n",
    "            try:\n",
    "                current_file = pd.read_excel(target_file)\n",
    "                new_file = pd.read_excel(downloaded_content)\n",
    "                \n",
    "                if not current_file.equals(new_file):\n",
    "                    if versioning_enabled:\n",
    "                        version(target_file)\n",
    "                    return True \n",
    "                else:\n",
    "                    print(f\"File {target_file} is unchanged.\")\n",
    "                    return False\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error comparing files: {e}\")\n",
    "                return False\n",
    "        elif target_file.endswith('.txt'):\n",
    "            # print(f\"Opening target_file: {target_file}\")\n",
    "            # print(f\"Opening downloaded_content: {downloaded_content}\")\n",
    "            # content is being\n",
    "            with open(target_file, 'r') as current_file:\n",
    "                if current_file.read() != downloaded_content.decode('utf-8'):\n",
    "                    if versioning_enabled:\n",
    "                        version(target_file)\n",
    "                    return True  \n",
    "                else:\n",
    "                    print(f\"File {target_file} is unchanged.\")\n",
    "                    return False\n",
    "        else:\n",
    "            print(f\"Skipping file {target_file} as it is not an Excel or text file.\")\n",
    "            return False\n",
    "\n",
    "    # Function that downloads all/chosen files\n",
    "    if documents == 'All':\n",
    "        for path, item_id in file_index.items():\n",
    "            downloaded_content = download_file(library_id, item_id)\n",
    "            if downloaded_content:\n",
    "                target_file = os.path.join(out_dir, path)\n",
    "                print(target_file)\n",
    "                print(f\"Downloading: {target_file}\")\n",
    "                if os.path.exists(target_file):\n",
    "                    if file_comparison(downloaded_content, target_file):\n",
    "                        os.makedirs(os.path.dirname(target_file), exist_ok=True)\n",
    "                        with open(target_file, 'wb') as f:\n",
    "                            f.write(downloaded_content)\n",
    "                            print(f\"Downloaded and saved: {target_file}\")\n",
    "                else:\n",
    "                    os.makedirs(os.path.dirname(target_file), exist_ok=True)\n",
    "                    with open(target_file, 'wb') as f:\n",
    "                        f.write(downloaded_content)\n",
    "                        print(f\"Downloaded and saved: {target_file}\")\n",
    "    else:\n",
    "        for doc in documents:\n",
    "            if doc in file_index:\n",
    "                downloaded_content = download_file(library_id, file_index[doc])\n",
    "                if downloaded_content:\n",
    "                    save_path = os.path.join(out_dir, doc)\n",
    "                    if os.path.exists(save_path):\n",
    "                        if file_comparison(downloaded_content, save_path):\n",
    "                            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                            with open(save_path, 'wb') as f:\n",
    "                                f.write(downloaded_content)\n",
    "                                print(f\"Downloaded and saved: {save_path}\")\n",
    "                    else:\n",
    "                        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                        with open(save_path, 'wb') as f:\n",
    "                            f.write(downloaded_content)\n",
    "                            print(f\"Downloaded and saved: {save_path}\")\n",
    "            else:\n",
    "                print(f\"File not found in SharePoint: {doc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0e67f7a-dda6-481f-8fb2-9338655e57f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c68d50e-4465-47bd-9238-e48ae7f34851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "download_files_from_sharepoint(\n",
    "    tenant_id='887a239c-e092-45fe-92c8-d902c3681567',  \n",
    "    client_id='622b5c85-96fe-4c12-95fe-2cfced5f0f73',  \n",
    "    scope=\"sharePoint_conn_tax_proj\", \n",
    "    key=\"Sharepoint_graph_api_key\",  \n",
    "    out_dir='Folder1', \n",
    "    site_name='GenAiProjectSandbox', \n",
    "    document_library='00_Guidelines', \n",
    "    documents=['investment_drivers.xlsx'], \n",
    "    versioning_enabled=True,  \n",
    "    structured=True,  \n",
    "    vers_dest='Folder1' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe340aa-9bc8-450d-91d5-3e755a61b4fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "file_path = 'Folder2/investment_drivers.xlsx'\n",
    "old_vers_path = 'versions/investment_drivers_20250422_134829.xlsx'\n",
    "df = pd.read_excel(old_vers_path, dtype={'rule_name': str})\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ecc8ec-2c4f-45dd-a0fe-9e4c615fecfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Sharepointdownloader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
